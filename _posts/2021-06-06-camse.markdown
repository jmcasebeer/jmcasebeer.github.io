---
layout: page
title: Cost Aware Speech Enhancement
description: Train a model that learns how many microphones to look at before producing an output.
img: /assets/img/camse/camse.png
permalink: /projects/camse
date: 2021-06-06
---

In this paper, we present a method for jointly-learning a microphone selection mechanism and a speech enhancement network for multi-channel speech enhancement with an ad-hoc microphone array. The attention-based microphone selection mechanism is trained to reduce communication-costs through a penalty term which represents a task-performance/ communication-cost trade-off. While working within the trade-off, our method can intelligently stream from more microphones in lower SNR scenes and fewer microphones in higher SNR scenes. We evaluate the model in complex echoic acoustic scenes with moving sources and show that it matches the performance of models that stream from a fixed number of microphones while reducing communication costs.

<center>
<img src="/assets/img/camse/camse.png" alt="drawing" width="75%"/>
</center>

Motivated  by the fact that in a multi-channel scenarios not all microphones will be required to achieve good performance. So, we can reduce complexity by only processing data from a subset of devices. We construct a toy scene below where a speech source moves through an ad-hoc array.

<center>
<img src="/assets/img/camse/camse_toy_scenes.png" alt="drawing" width="100%"/>
</center>

However, selection is not a differentiable operation making it difficult to use with a neural network. To get around this we formulate the [Adaptive Compute Time RNN](https://arxiv.org/abs/1603.08983) originally proposed by Alex Graves. Our reformulation iteratively requests new data in addition to updating it's hidden state. We show the original Adaptive Compute Unit on the top in blue and our modification below in yellow.

<center>
<img src="/assets/img/camse/compute_to_select.png" alt="drawing" width="50%"/>
</center>

Using this adaptive request unit, we can run a speech enhancement model on an ad-hoc microphone array and adaptively decide how many microphones to stream from. Below, we depict the model at particular time step after having processed three microphones. The attention unit aggregates all information from the requested microphones and the scoring unit produces a value which we compare to a budget. If the value is within the budget then another microphone is requested. Otherwise, the model ceases requesting and moves on to the next point in time. The model is encouraged to request fewer microphones by minimizing the remainder, r. Minimizing this remainder maximizes the scores assigned to all previous sensors which encourages the model to exhaust it's budget sooner.

<center>
<img src="/assets/img/camse/ponder_diagram.png" alt="drawing" width="100%"/>
</center>

The full model is made up of a multi-channel encoding stage, a request and aggregation stage, and a single channel decoding stage. The pipeline is shown in the model figure above. We tested this model in simulated echoic rooms with moving noises and source like the one shown above.

<center>
<img src="/assets/img/camse/camse_model.png" alt="drawing" width="100%"/>
</center>

The novelty of our method is two-fold. First, using a learning-based mechanism for microphone selection and second, learning that mechanism jointly with the enhancement network. We think there are many potential applications for these adaptive request units and hope to continue investigating them. Below are some randomly selected enhancement results from the trained network.

<p align="center">
  <strong>Randomly Selected Demos</strong>
</p>

<table border="1" style="margin:10px auto; width:85%">
  <tr>
    <td> </td>
    <td>Random Scene 1</td>
    <td>Random Scene 2</td>
  </tr>
  <tr>
    <td>Clean</td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/clean.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/clean_1.wav" /></audio></td>
  </tr>
  <tr>
    <td>Microphone 1</td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_0.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_0_1.wav" /></audio></td>
  </tr>
  <tr>
    <td>Microphone 2</td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_1.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_1_1.wav" /></audio></td>
  </tr>
  <tr>
    <td>Microphone 3</td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_2.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_2_1.wav" /></audio></td>
  </tr>
  <tr>
    <td>Microphone 4</td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_3.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/noisy_3_1.wav" /></audio></td>
  </tr>
  <tr>
    <td>Prediction </td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/pred.wav" /></audio></td>
    <td><audio controls style="width: 200px;"><source src="/assets/audio/camse/pred_1.wav" /></audio></td>
  </tr>
</table>


Our paper is available on arxiv [here](http://arxiv.org/abs/2011.07348). Our code is available on GitHub [here](https://github.com/jmcasebeer/cost_aware_enhancement){:target="\_blank"}!
